# IRV AutoPackaging

FastAPI + Celery + Individual processors per Dataset for DAG ETL Pipeline

Encompasses API and backend-processing to generate / manage datapackages associated with boundaries

## Data Processing

Celery-based worker running DAG's generated through API request

__TODO Docs__:

* Celery
* Dag structure
* Processors (Internal) - Boundary and Provenance 
* Processors (Core) - dev deployment and testing
* Secrets required for Processors
* Concurrency
* Duplicate Task submission and execution

### Running Locally:

```bash
celery --app dataproc.tasks worker --loglevel=debug --concurrency=1
```

Running using Docker:

```bash
docker-compose up dataproc
```

### Testing

Integration tests in `tests/dataproc/integration/processors` all run standalone (without Redis / Celery), but you'll need access to the source data for each processor (see above).

__NOTE__: Test for geopkg (test_natural_earth_vector) loading include a load from shapefile to postgres - the API database is used for this test and configured user requires insert and delete rights on the api database for the test to succeed.

```bash
# Run tests locally
python -m unittest discover tests/dataproc
# Run tests in Docker
docker-compose run test-dataproc
```

## API

API Covering boundaries, processors, packages and processing job submission against packages.

The source of truth of package data for the API is the configured filesystem.

The API uses boiundaries loaded into a configured postgres database (see below).

### Boundaries

Boundaries are sourced by the API from a local PostGIS table.

To load the boundaries in the correct form you can use the helper script: `tests/data/load_boundaries.py <geojson filepath> <name column> <long_name column> <wip table true/false>`.

The boundaries table schema is managed by Alembic can be found under `api/db/models/boundary.py`.

__NOTE__: API Integration tests require a Db user who has RW access to this table.  
__NOTE__: The configured API database will be wiped during running of the integration tests and loaded with test-boundaries.

### Running Locally:

```bash
uvicorn api.main:app --host 0.0.0.0 --port 8000
```

Running using Docker:

```bash
docker-compose up api
```

### Documentation

#### OpenAPI

* Run the app as above
* Navigate to http://<host>:<port>/openapi.json

#### ReDoc

* Run the app as above
* Navigate to http://<host>:<port>/redoc

### Testing

__NOTE__: API and Dataproc tests required access to shared processing and package folders for assertion of processor outputs.

__NOTE__: API tests will add and remove boundary test-data to/from the Db during execution.

__NOTE__: API tests will add and remove package data to/from the configured packages directory during execution.  Temporary processing data for `natural_earth_raster` will also be generated and removed from the configured processing backend folder.

__NOTE__: Dataproc will add and remove package data to/from the packages source tree during execution.  Processors will also remove data from their configured temporary processing directoryies, depenign on how they are configured.

__NOTE__: Individual processor integration tests require access to source data to run successfully.

#### Locally

Ensure the Celery Worker, Redis, PG and API service running are running somewhere (ideally in an isolated environment as assets will be generated by the tests) if you want to run the integration tests successfully.

Ensure you also have `AUTOPKG_LOCALFS_STORAGE_BACKEND_ROOT_TEST` set in the environment, so both API and Celery worker can pickup the same package source tree

```bash
export AUTOPKG_DEPLOYMENT_ENV=test
# Run API
uvicorn api.main:app --host 0.0.0.0 --port 8000 --reload

# Run Worker 
celery --app dataproc.tasks worker --loglevel=debug --concurrency=1

# Run tests locally
python -m unittest discover tests/dataproc
python -m unittest discover tests/api
```

#### Docker

Alter deployment env in .env file: `AUTOPKG_DEPLOYMENT_ENV=test`

```bash
docker-compose up -d db redis api dataproc
docker-compose run test-api
```

#### Localfs or S3 Backend

Altering deployment env with `AUTOPKG_STORAGE_BACKEND=awss3` or `AUTOPKG_STORAGE_BACKEND=localfs` will also mean tests run against the configured
backend.

__NOTE__ awss3 integration tests require supplied access keys to have RW permissions on the configured bucket.

```bash
export AUTOPKG_STORAGE_BACKEND=awss3 && python -m unittest discover tests/dataproc
```

To conduct a simple set of tests (no processors) against S3 to check basic functionality you can use:

```bash
export AUTOPKG_STORAGE_BACKEND=awss3
python -m unittest tests/dataproc/integration/storage_backends/awss3/test_awss3.py
```

### PG Schema Management with Alembic

The database schema is managed through Alembic.  The following serves as a guide to basic usage for extending the schema - refer to https://alembic.sqlalchemy.org/en/latest/ for more information.

#### Schema Updates

* Make changes as required to models
* From within the autoppkg/api folder run the following to auto-generate an upgrade/downgrade script:

```bash
alembic revision --autogenerate -m "Added Boundary Table"
```

__NOTE__: CHECK the script - remove extransous operations (in particular those relating to spatial-ref-sys)

* When reaady run the following to upgrade the database:

```bash
# Ensure the AUTOPKG_POSTGRES_* env variables are set (see below)
cd api
alembic upgrade head
```

## Deployment Environment

```bash
export AUTOPKG_POSTGRES_USER= # Used for API Boundaries in Prod (and test natural_earth_vector processor in Worker)
export AUTOPKG_POSTGRES_HOST= # Used for API Boundaries only (and test natural_earth_vector processor in Worker)
export AUTOPKG_POSTGRES_PASSWORD= # Used for API Boundaries only (and test natural_earth_vector processor in Worker)
export AUTOPKG_POSTGRES_PORT= # Used for API Boundaries only (and test natural_earth_vector processor in Worker)
export AUTOPKG_POSTGRES_DB= # Used for API Boundaries only (and test natural_earth_vector processor in Worker)
export AUTOPKG_CELERY_BROKER= # Used for Worker only
export AUTOPKG_CELERY_BACKEND= # Used in API and Worker
export AUTOPKG_CELERY_CONCURRENCY= # Celery worker concurrency - dataproc only
export AUTOPKG_TASK_LOCK_TIMEOUT=600 # Secs - Duplicate task lock timeout (blocks duplicate processors from executing for this time)
export AUTOPKG_TASK_EXPIRY_SECS=3600 # Secs before tasks expire on Celery - dataproc only
export AUTOPKG_STORAGE_BACKEND= # Storage backend to use for final packages (see additional backend-specific flags below for more info).  Used in API and Worker
export AUTOPKG_LOCALFS_STORAGE_BACKEND_ROOT= # Path to root-directory for packages.  Used in API and Worker
export AUTOPKG_LOCALFS_PROCESSING_BACKEND_ROOT= # Path to root-directory for local interim processing data.  Used by Worker only
export AUTOPKG_LOCALFS_STORAGE_BACKEND_ROOT_TEST= # Path to root-directory for packages when running integration tests.  Used in API and Worker
export AUTOPKG_LOCALFS_PROCESSING_BACKEND_ROOT_TEST= # Path to root-directory for local interim processing data when running integration tests.  Used by Worker only
export PACKAGES_HOST_URL= # URL to the hosting engine for package data, e.g. http://localhost
```

### Local FileSystem Storage Backend

```bash
export AUTOPKG_STORAGE_BACKEND=localfs
export AUTOPKG_LOCALFS_STORAGE_BACKEND_ROOT= # Base-folder for packages
export AUTOPKG_LOCALFS_STORAGE_BACKEND_ROOT_TEST= # Base path for integration-test package data
```